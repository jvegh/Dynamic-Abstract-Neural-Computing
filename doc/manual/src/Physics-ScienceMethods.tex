% How modeling, in general, works 

\section{Science methods\label{sec:Single_ScienceMethods}\label{sec:Physics-Abstraction}}




%\subsection{Abstractions\& approximations\label{sec:Physics-Abstraction}}
For centuries, science developed its methods for deriving abstract concepts
by reducing the features of a real object to an abstract one that cannot be reduced further, such as mass or charge.
Ions are an exception: \textit{the ions are charge and mass simultaneously, without a further possibility of reduction}.
The consequences of this item are listed in Table~1 of paper~\cite{ThermodynamicAPDrukarch:2022}
as the "Electrical vs Mechanical" dichotomy.
Science has derived laws for the forces acting on those abstract objects, such as Newton's universal law of gravitation and Coulomb's law of electricity. Then one could apply Newton's laws of motion. Experience shows that the generated forces, regardless of their origin, can be summed, and one can apply the laws of motion using the resultant force (see below for a discussion of the resultant force and its application in biology).


Science disciplines attempt to discover the infinitely complex nature  
in a disciplinary approach.
To describe a \textit{well defined} range of phenomena, we use approximations and omissions,
and we create \hypertarget{Abstractions}{abstractions} which can then be described by known laws using
the universal language of mathematics. We use the abstraction "charge"
and "charge carrier" for electrons, protons, ions, etc., and we can describe the
electricity-related abstract features of the carriers.
We must not forget, however, that \textit{those laws have been
derived for abstractions based on approximations and omissions}, and so they also \textit{have
their range of validity}. To apply laws from different fields of science,
we must scrutinize whether all laws we use are applied within their range of validity.


Science, unfortunately, is separated into classical and modern science
based on whether the theoretical description assumes infinitely fast
interaction (the Newtonian model) or acknowledges the finite interaction
speed (the Einsteinian model). However, \textit{the finite interaction speed
is erroneously associated with the speed of light and frames of reference
moving relative to each other with speeds approaching the speed of
light}. Assuming that the interaction speed is finite  is sufficient to
build up the special theory of relativity \cite{ModernRelativity:1993} (using the speed
of light as the value for its external parameter).
Still, the Minkowski-mathematics \cite{Minkowski:1908} behind the
special theory of relativity works with any speed parameter $c$.
The same mathematics describes technical  \cite{VeghRevisingClassicComputing:2021}
and biological \cite{RoleOfInformationTransferSpeed:2022} computing
systems, where there are no moving reference frames, but the finite
interaction speed has noticeable effects on the operation of the system.

Another main source of confusion is that the phenomena happen in a limited region of space, and we study processes (in a period instead of a moment) where the environment is not "infinitely far" from the studied object and \textit{the studied process interacts with its environment}. We must consider that the resources are finite.

Science has a well-established method for deriving its laws. It groups resemblant phenomena to major disciplines and they branch along even more resemblance to sub-discipline.
The hierarchy is not strict, given that "{nature is not interested in our separations, and many of the interesting phenomena bridge the gaps between fields.}" ~\cite{FeynmanThinking:1980}".
Mathematics is a discipline on its own right. It abstracted its notions from nature's concrete objects and phenomena, and its abstract methods serve as a solid base for the fellow sciences and are frequently used to describe new discoveries.
It is commonly accepted that physics is  the very basic tool to study nature at some elementary level.
Chemistry, at another level of abstracting the nature, is based upon its laws, and constructs its laws for the phenomena it researches.
The laws of physics are borrowed and respected, and the laws of chemistry cannot conflict with the laws of chemistry.
Biology is the discipline studying living matter, and is considered to be an even higher level abstraction, with its own laws. These laws, as expected, cannot conflict with the laws of chemisty and physics.
Howevever, it looks like some laws, mainly the ones belonging to thermodynamics, are are not perfect for describing biological phenomena. Especially, the life itself seems to contradict 
the second law of thermodynamics. E.~Schrödinger suspected that 
some non-ordinary laws are to be derived to describe life.

Classical physics is simplifying the things until can attribute 
a single substance (such as mass or charge) to the tested object
and in its laboratories derives accurately the laws and determines
the attibutes of the substances. As E.~Schrödinger has drawn the attention to, "the construction [of living matter] is different from anything we have yet tested in the physical laboratory".
Science experienced many times that the approximation do not
describe some phenomena accurately. At that times, scrutinizing the 
fundamental assumptions (such as whether the \indexit{Galillei's relativity principle} remain valid when approaching the speed of light,
or the matter remains continuous when decreasing extremely the 
tested sample) has led to establishing new scientific fields,
such as theories of relativity or theory of quantum mechanics.

\quotationbox{
"living matter, while
not eluding the 'laws of physics' as established up to date, is likely to involve 'other laws of
physics' hitherto unknown, which, however, once
they have been revealed, will form just as
integral a part of this science as the former."~Schrödinger@1944~\cite{Schrodinger:1992}
}

\subsection{Dichotomies in modeling\label{sec:Physics-Dichotomies}}

The paper~\cite{ThermodynamicAPDrukarch:2022}, which, in the context of this section, tends to be the precursor of our paper, sheds light on some remarkable dichotomies,
scrutinizing which can result in "a sound basis for unification of the physics of nerve impulses".
By deeply agreeing with that they have a "potential impact on our understanding of (the physical nature of) neuronal signaling", we list and uncover more dichotomies; furthermore, we uncover their fundamental,
partly philosophical reasons.

\subsubsection{Electrical vs Mechanical\label{sec:Physics-ElectricalMechanical}\label{sec:Physics-SlowCurrent}}
During centuries, science developed its method to derive abstract concepts
by reducing the features of a real object to an abstract one that cannot be reduced further, such as mass and charge. It derived laws for the forces acting on those abstract objects,
such as Newton's universal law of gravitation and Coulomb's law of electricity.
\index{Coulomb's law}
Then one could apply Newton's laws of motion. Experience shows that the generated forces, independently of their origin, can be summed and one can apply
the laws of motion by using the resultant force.
Ions are special from the point of view of reducing the 'material point'
to one single abstraction: \textit{the ions are charge and mass simultaneously, without a further possibility of reduction}.
Notice that in the electrical abstraction, no mass is present,
so one can use the equations assuming 'instant interaction',
which leads to non-physical explanations of the observations (such as 'delayed current').
In the mechanical abstraction, mass is to be moved, 
making the finite-speed interaction evident. The same physical phenomenon,
the interaction (or movement) of ions is described using an 'infinite' electrical speed and 
a million times less mass propagation speed, respectively, which leads
to an unresolvable discrepancy, given that physics is not prepared to handle
different speeds in the same interaction event~\cite{VeghNon-ordinaryLaws:2025}.
This item is listed in Table~1~in~\cite{ThermodynamicAPDrukarch:2022}
as the "Electrical vs Mechanical" dichotomy.

%\subsubsection{Slow current\label{sec:Physics-SlowCurrent}}

\textit{To deliver a
current, one needs moving charged particles that need acting of some
external (electric, magnetic, or chemical) force or a mixture of forces.
We have speeds of 
\gls{EM} interaction, thermal motion of the charge
carriers, macroscopic current, drift speed}, and their mixing, simultaneously,
in the same phenomenon. In the theoretical description of
processes, instant interaction (i.e., the abstraction of non-physical,
infinitely large interaction speed) is used in most cases. In the
cases when absolutely needed, the generic notion of "speed" is
used without specifying which one of the mentioned speeds it means.

The low speed of ions in electrolytes introduces further problems.
Any change in the local value of the state variables will be seen
by the farther parts of the cell with a delay
The material transport represents simultaneously mass and charge,
so the transport itself changes the gradient. This process keeps the entire volume of the electrolyte in (more or less intensive) continuous change. Furthermore, the biological objects inside the cell
can absorb ions and charge up. With their potential, they change the local gradients, accelerating or decelerating the ions. 
Not to mention that biological objects can be active in the sense that 
(depending on the environmental conditions) they can let ions from one separated volume part to the other. 

\subsubsection{Instant interaction\label{sec:Physics-InstantInteraction}}
Physics notoriously suffers from the lack of handling
\hypertarget{MixingSpeeds}{different simultaneous interactions}; facing such a case leads to misunderstandings, debates and causality problems.
Such a famous case is the interaction speed of entanglement.
In that time, E. Schrödinger introduced his famous Law Of Motion in quantum-mechanics entirely analogously
as I. Newton introduced his Laws of Motion.
Similarly to the Newtonian 'absolute time', the quantum mechanical interaction is supposed to be 'instant'
(this is the price for having 'nice' equations in classical and quantum mechanics),
i.e., its speed is supposed to be infinitely high.
However, at that time was already known that the electric interaction (propagation of electromagnetic waves) is finite,
so if an object has quantum mechanical interaction (aka entanglement) 
and electrical interaction, the corresponding forces start at the same time, but arrive at the other object at different times.
The entanglement arrives instantly, the electromagnetic effect arrives at the time we can calculate from the interaction speed and the spatial distance of the objects.
This leads to causality problems: the effect of the two interactions of photons entangled earlier in an exploded supernova should be measured at two different times; meaning a "spooky remote interaction" as A. Einstein coined,
and leads to contradictions such as the Einstein–Podolsky–Rosen paradox.
Actually, the issue roots in the improper handling of mixing interaction speeds:
the Schrödinger-equation introduces the infinitely large interaction speed,
while the
\gls{EM}
interaction has finite speed.


\textit{The confusion and question marks in connection with describing the life by science mostly arise from the interpretation of
	notion 'speed' in physics.}
When discussing the underlying physical laws, we
go back to the very basic physical notions instead of taking over the
approximations and abstractions
used in the \textit{classical physics for non-biological matter} and
less complex interactions.
As we emphasized many times, we construct laws and conclusions based
on somewhat simplified abstractions about nature, in all fields of science.
The notions and laws depend
on the circle of phenomena we know and want to describe.
The Newtonian and Einsteinian worlds are
basically distinguished by considering \textit{speed dependence} that actually means \textit{explicite time dependence}.
Interesting consequences are that in the Einsteinian world,
the mass is not constant, the time and space are not absolute, and so on.
We can be prepared to some similar counter-intuitive experiences in physiology "we must be prepared to find it working in a manner that cannot be reduced to the \textbf{ordinary} laws of physics"\cite{Schrodinger:1992}.
Here we scrutinize the basic notions and discover some differences
between physics and biology
as consequences of the required different abstractions and approximations.



During our college studies, we mentioned that light is an electromagnetic
wave with a vast but finite propagation speed. Still, we forgot to
highlight that, at the same time, it is the propagation speed of the
electric (, and magnetic, and gravitational) interaction force fields
as well. The effect of "Retarded-Time Potential" is also known
in physics and communication engineering. Algorithms "marching-on-in
time" and "Analytical Retarded-Time Potential Expressions" are
derived to handle the problem; for a discussion, see \cite{RetardedTimePotential:2011}.
\index{telegrapher's equations}
Telegrapher's equations (unfortunately, also used to describe biological signal transfer)
explicitly assume a finite propagation speed millions of times slower
than the (implicitly assumed) \gls{EM} 
interaction's. The issue is not confined to large
distances: designers of micro-electronic devices also must consider
the effect: they introduced clock time domains and clock distribution
trees; see, for example \cite{WiringDominance:2019, VeghRevisingClassicComputing:2021}.


Science uses 'instant' in the sense that one interaction
is much faster than the process under study; we consider the faster interaction as instant.
\index{instant interaction}
The approach of classical science is based on the oversimplified
approximation that the interaction speed is \emph{always} much higher
than the speed of changes it causes and that the processes can \emph{always}
be described by a single stage. In our approach, for biology, we put together a
\textit{series of stages} to describe the observed complex phenomena, where the stages provide input and output
for each other, involve more than one interaction speed, and use per-stage-valid
approximations. We simplify the approximations by omitting the less
significant interactions and introduce ideas for accounting for the different
interaction speeds. This way, we reduce the problem to a case that
science can describe mathematically. \emph{This procedure is fundamentally different
from applying some mathematical equations derived for an abstracted
case of science to a complex biological phenomenon without validating
that we use the appropriate formalism}.



\paragraph{Speeds\label{sec:Physics-Speeds}}

The role of speed and time, particularly in
the context of an object's changing location over time, has long held
a mystique in the realm of scientific discovery (and recently returned
to be mystic again in cosmology). This intrigue can
be traced back to historical debates, such as \href{https://en.wikipedia.org/wiki/Zeno's_paradoxes}{Zeno's paradoxes}. The
acknowledgment that an object's movement speed can influence our
observations is a topic that has sparked significant scientific discourse
over the years. In this section, we aim to draw parallels between
the historical debate on the finite speed of light and its contemporary
implications in various scientific disciplines, such as the finite
speed of ionic currents in biology.

It has been a long-standing mystery that interactions with different speeds play
their role \textit{simultaneously}. The issue forces researchers to give
non-scientific explanations to everyday phenomena only because \textit{they
routinely assume that the interactions have the same speed, and they use
the laws about strictly pair-wise interactions}. They have no choice: 
there is no formalism to handle non-equal speeds. 

We need different abstractions (finite-speed interaction in
modern physics) for different phenomena that require different mathematical
handling, which is not as simple and friendly. The \textit{speeds of
observation} and propagation of electric fields remain the same in
biology, and it is easy to extrapolate, mistakenly, that \textit{all}
interactions have infinitely large interaction speeds. However, also
\textit{slow} interaction speeds exist, furthermore, \textit{different interaction speeds
can intermix in the same phenomenon. Neglecting that effect introduces
the need to assume fake mechanisms and effects} for explaining
some details; which are naturally explained by assuming finite interaction
speeds and their combinations. More discussion in section~\ref{sec:Physics-ThermodynamicsMixingSpeeds}.



\paragraph[In observations]{Speed of light\label{sec:Physics-SpeedLight}}


In 1676, the Danish astronomer Ole Rømer was making meticulous observations
of Jupiter's moon Io and concluded not only that the speed of light
is finite, but he measured its value with sufficient accuracy. Rømer
never published a formal description of his method, possibly because
of the opposition of his bosses, Cassini and Picard, to his ideas.
Cassini knew Rømer's idea and the measurement data. However, instead
of accepting the finite value of the speed of observation, he made
periodic corrections to the tables of eclipses of Io to take account
of its irregular orbital motion: \textit{
\hypertarget{ResettingClock}
{periodically resetting the clock}
}.
The speed of light must remain infinitely large.
\index{clock reset}

However, the theory of finite speed quickly gained support among other
natural philosophers of the period, such as Christiaan Huygens and
Isaac Newton~\cite{Roemer:1676}. Although Newton surely knew that
the observation speed was finite, in his "Philosophiae Naturalis
Principiai Mathematica"~\cite{NewtonPrincipia:1687}, published
in 1687, he decided to refer to observations that they happen "at
the same time" despite knowing that what we observe at the same
times, happen at different times. Using instant interaction results
in "nice'" mathematical laws and enables us to describe most of
nature's experiences with sufficient accuracy.
%
Einstein, in 1905, discovered~\cite{EinsteinSpecial:1905} that the speed of observation (in moving
reference frames) may play a decisive role in interpreting scientific
phenomena. The results he derived using Minkowski-coordinates~\cite{Minkowski:1908} were
counter-intuitive, with many unexpected consequences. Instead of introducing
improvement(s) or correction(s) to the existing classic principles
and methods, he introduced a new principle: the finite (limiting) interaction speed.
The \textit{disciplinary analysis of the reception of
Minkowski's Cologne lecture reveals an overwhelmingly
positive response on the part of mathematicians, and a decidedly mixed
reaction on the part of physicists}~\cite{Minkowski100:2008}
has turned to the exact opposite. Today, physics generally accepts
the description, that is, the existence of finite interaction speed
(resulting in the birth of a series of modern science disciplines).
However, other science disciplines, including biology and computing
science, refute (or at least do not use) it; despite that its effects are evident.

\paragraph[In neuroscience]{Speed in neuroscience\label{sec:Physics-SpeedNeuro}}

Helmholtz, in 1850, sent a short report off to the Academy~\cite{ HelmholtzReport:1850}
"I have found that a measurable time passes when the stimulus exerted
by a momentary electric current on the hip plexus (Hüftgeflecht) of
a frog propagates itself to the nerves of the thigh and enters the
calf muscle." His teacher "had thought that the speed of nervous
conduction might be in excess of the speed of light and could probably
never be measured. Helmholtz's father, on hearing of the experiment
and the surprisingly slow measured speed, wrote to his son that he
would as soon believe this result as that one can see the light of
a star that burned out a million years ago"~\cite{HelmoltzHistory:1851}.

With the development of measurement technology, it became evident
that finite speed is a general feature of the "nervous connection".
(Somehow, "the speed of nervous conduction" has been renamed to
"conduction velocity", neglecting the clear distinction that physics
makes between the two wording.) With the dawn of instrumental electronics
and computing, the McCulloch-Pitts model~\cite{LogicalCalculusMcCulloch:1943}
\index{neuron model!McCulloch-Pitts}
introduced the picture that the brain can be modeled by a network
of simple perceptron nodes connected by wires; that is, it comprises
a two-state equipotential membrane connected with perfect wires. The
experimental research also quickly (re-)discovered that those wires
forward signals in a particular way; the speed of the potential wave
%(and that of the attached "transversal ion current")
is finite.
Furthermore, \textit{the axons are not equipotential during transmission}.
Although its structure is practically identical with axons, \textit{biology
assumes that, unlike an axon, the membrane remains equipotential during
its operation, although the evidence shows the opposite}: 'the action potential spreads as a traveling wave from the initial site of depolarization
to involve the entire plasma membrane' \cite{MolecularBiology:2002}.
\index{Action Potential}

When seeing that assuming an equipotential membrane was wrong and
a single equipotential surface (in other words, classic physics' instant
interaction) cannot describe neurons adequately, multi-compartment
models (typically comprising equipotential cylinders with different
potentials) have been introduced \cite{MathNeuroscience:2010}. (Notice that it is a consequence of the wrong oscillator model hypothesized by Hodgkin an Huxley: the membrane is modeled as a series of resistors and capacitors.) Then
(forgetting that Ohm's Law is valid only for classic physics's 'instant
interaction', furthermore, that no external potential is connected
to either of the compartments, and no charge is present at the beginning,
except at the input of the first compartment), the individually
\index{neuron model!multi-compartment}
equipotential
compartment pieces were connected by individual resistors. This model shows that
the more compartments,
the better the agreement (accuracy) with experiments.
It happens because the shorter is the size of the compartment
(approaches a differential equation), the less noticeable is the deviation
from the true non-equipotential surface. This conclusion means that
charging the capacitance attached to the compartment takes time, resulting
in a delayed distributed current. Using infinitely many
compartments, we would arrive at the differential equations describing
a delayed distributed current on the surface of the non-equidistant
membrane. However, biology did entirely not give up its position.
It admitted that membrane current exists, but only between compartments,
and its speed must be infinite (or, at least, the speed of 
\index{delayed current} \gls{EM}
interaction). However, at least the compartment pieces must be equipotential.
Instead of fixing the wrong hypothesis, biology is "periodically
resetting the clock". Instead of accepting that the charged ions
represent a "slow current" (compared to the "fast current" represented by electrons and their charge transmission method), biology introduced changing conductance, delayed current, rectifying current, 

%\hypertarget{PHYSICS_SPEEDS_FINITE}{}
\paragraph[In interactions]{Finite-speed interactions\label{PHYSICS_SPEEDS_FINITE}}

When speaking about speed, especially about the speed of charged objects
inside biological objects, one needs to consider microscopic and macroscopic
levels of understanding. On the boundaries of the two levels, we need
to make distinctions between different kinds of speeds, among others
(in units of $m/s$), the propagation speed $10^{8}$ of the electric
field (aka potential gradient), the speed $10^{5}$ of thermal motion and potential-accelerated motion, the
apparent speed of current (potential-assisted speed of a macroscopic stream, both
in metals and electrolytes; mainly due to the repulsion of nearby ions in the stream) $10^{1}$,
 for ion current inside a neuron
(see Fig. 1 in ~\cite{ActionPotentialGenerationNatrium:2008}) $10^{-2}$;
diffusion speed of electrons in a wire $10^{-4}$, drift speed
 of the individual carriers in aqueous solutions $10^{-7}$; and
of ions moving in a narrow tube filled with viscous liquid $10^{-8}$. Fortunately,
in most \textit{but not all} cases, different mechanisms
(such as the Grotthuss mechanism or free electron model; for a review,
see \cite{GrotthussMechanism:2023}) at the level of microscopic
structure help to create the illusion of a high macroscopic propagation
speed (million times higher than the speed of its microscopic carriers).
\textit{The same carrier can have macroscopic speeds differing by orders of
magnitude, depending on the context};  see a biological example at
%\hyperlink{MODELING_SINGLE_ION_CHANNEL}
{ion channels}.
\index{ion channel}
When more than one of those
speeds plays a role in the phenomenon we study, we must carefully
consider its context and prepare for handling \textit{fast}
and \textit{slow} effects, furthermore, their mixing.


When an object can interact with another in a way abstracted by science
as more than one interaction type, we need to find the relation (the 'extraordinary' law) between
them. Such a famous case is electricity and magnetism.
 Their interrelation
is defined by the Maxwell equations: how an electrical field creates
\index{Maxwell equations}
a magnetic one and vice versa (notice that the law is about their
\emph{space derivatives, aka space gradients} instead of the entities themselves). 
While we understand that the speeds of electromagnetic and gravitational
interactions are finite, we can use the 'instant interaction' approximation
in classical physics because one effect of the first particle
reaches the second particle simultaneously with the other effect,
leading to the absence of a time-dependent term in the mathematical
formulation.

An apparently
similar case is found in electrodiffusion, where ions can be abstracted
as mass and charge, one belonging to thermodynamics and the other
to electricity. There is, however, an essential difference between
those cases: the interaction speeds are the same in the first case
(moreover, in the spirit of classical physics,
the interactions are instant) and differ by several orders of magnitude
in the second one. 
 Of course, the Maxwell equations can be nicely solved 
and modeled also for biology
if one introduces~\cite{ElectromagneticNeuron:2018}  
that the axial currents have the same speed (BTW: which was measured as $20\ m/s$) as the electric and magnetic waves,
furthermore the longitudinal current is (?)defined(?) to have no attenuation. 
Furthermore, it is likely also defined that current needs no driving force 
and this is why the positive and negative ions flow in the same direction.  It is really
a novel paradigm leading to "(mis)\href{https://www.nature.com/articles/s41598-018-31054-9}{understanding cell interactions}", but definitely describes some alternative nature.



\paragraph[In sciences's laws]{Speed in laws of science\label{PHYSICS_SPEEDS_LAWS}}

Actually,  the famous Coulomb's Law is expressed as
\index{Coulomb's law}
\begin{equation}
\frac{F(t)}{Q_{1}}=k\frac{Q_{2}}{{\overrightarrow{r}}^{2}}\label{eq:CoulombTimeDependentSpaceTime}
\end{equation}
\noindent $\overrightarrow{r}$ is a space-time distance.
That is, in the Newtonian approximation, time is identical at all places,
so we used to omit it. 
However, physics knows also the notion of \textit{retarded time}.
\index{retarded time}
Considering the finite field propagation speed requires revisiting
the fundamental physical laws. (in a Lorentz-transformed form) should be written as
%
\begin{equation}
\frac{F(t)}{Q_{1}}=k\frac{Q_{2}}{r^{2}}(t{-\frac{r}{c}})\label{eq:CoulombTimeDependent}
\end{equation}
%
\index{Coulomb's law}

\noindent
The electrostatic field that charge $Q_1$ experiences
due to the finite propagation speed $c$ of the electric
field (or interaction) corresponds to that $Q_2$ at
a distance $r$ \textit{generated $\frac{r}{c}$ time ago}
($k$ is the constant describing the electric interaction). This term
has no role if the two charges do not change their position; similarly
to that in the special theory of relativity, only the relative movement
leads to complications. If the distance changes, its effect is so
tiny that the term can usually be omitted. So, our college knowledge
can serve as a good first approximation.


This speed term pops another law from classical physics into our minds:
Kirchhoff's junction rule. The law is perfect in the approximation
'instant interaction' that classical
physics uses, but not for biology.
\index{Kirchhoff's Laws}
First, because it expresses charge conservation, \textit{it
is invalid when charges are "created" inside biological objects}
(ions diffuse into the junction; see the role of ion channels in the
wall of membranes). Second, it is not valid for input currents arriving
with finite speeds into finite-size space regions, but it is valid for a single point in space-time
(in other words, in differential equation form). 
As we discuss in section~\ref{sec:Physics-Current},
using a wrong definition of current means assuming 'instant interaction', that is, that neural signals propagate with the speed of light.
The currents (and
the voltages), measured at two different points in space-time, are
different. Consequently, for extended objects (such as a line-like finite-size
neuron), it is valid only with a time delay
%
\begin{equation}
I_{out}(t)=-I_{in}(t-\Delta t)\label{eq:Kirchoff_biology}
\end{equation}
%
The time delay in biology is in the $1\ msec$
range.
\textit{We must not describe the axon or the membrane by the differential form the non-differential form Kirchoff equation:
	the input and the output currents flow at different times (the charge carriers need time to reach from input to output)}. \textit{We must not describe the axon or the membrane with the non-differential form of the  Kirchoff equation:
the input and the output currents flow at different times (the charge carriers need time to reach from input to output); only the differential
equation form expresses charge conservation} (furthermore, in the case of "producing" ions, even by the differential form is invalid).
For its exact interpretation see sections on
%\hyperlink{AXONAL_CHARGE_DELIVERY}
{axonal charge delivery}
and on the
%\hyperlink{MODELING_SINGLE_MEMBRANE_TRUE_CURRENT}
{true membrane current}, Fig. \ref{fig:The-ghost-image_AP},
and the text around it. \textit{Studying electric phenomena on structured
media, such as biological cells, needs much care}. We must not apply
laws derived from entirely different conditions (mainly metals).

\subsection{Other issues\label{sec:Physics-OtherIssues}}


\subsubsection{Closed volume\label{sec:Physics-ClosedVolume}}

The closed volume of the biological objects is also
a problem of finite resources. In biology, we cannot use the
fundamental assumption of physics that, although a field acts on the 
ion in question, the ion does not affect the field (the other ions generate).  The transferred ions decrease the field in the volume they departed from
and increase it in the volume where they arrived. Due to the 
field-dependent speed within the electrolyte, we must consider the autonomous
change between the microvolumes where the ion traverses. 
These processes are what E.~Schrödinger coined as "\textit{the construction is different from anything we have yet tested in the physical laboratory}"~\cite{Schrodinger:1992}. Consequently, measurements must be designed and carried out with care; the routine
methods used for measuring objects from inanimate nature,
cannot surely be applied to living objects.  




\subsection{Instant Interaction\label{sec:Physics-Instant}}


Those dichotomies are rooted in deeper layers of science.
Notice that in the electrical abstraction, no mass is present,
so one can use the equations assuming 'instant interaction',
which in biology led to using non-physical explanations of the observations (such as 'delayed current').
In the mechanical/thermodynamic abstraction, mass is to be moved, 
making the finite-speed interaction evident.
Classical physics is based on the Newtonian idea that space and time are absolute, so everything happens simultaneously. Consequently, when nature's objects
interact, it must be instantaneous; in other words, their
interaction speed is infinitely large. Furthermore, electromagnetic
waves with the same high (logically, infinitely high) speed inform the observer. This
self-consistent abstraction enables us to provide a "nice"
mathematical description of nature in various phenomena: the classical
science.

%In the first year of college, 
We learned that the idea resulted
in "nice" reciprocal square dependencies, Kepler's and Coulomb's
Laws. We discussed that the macroscopic phenomenon "current" is
implemented at the microscopic level by transferring (in different
forms) discrete charges; furthermore, that solids show a macroscopic
behavior "resistance" against forwarding microscopic charges. We also learned that \textit{without charge (and, without atomic charge carriers), neither potential nor current exists}. We did not learn, however,
that also thermodynamic forces can move the ions,
given that they have inseparable mass.


The low speed of ions in electrolytes introduces problems.
The same physical phenomenon,
the interaction (or movement) of ions is described using an 'infinite' electrical speed and 
a million times less mass propagation speed, respectively, which leads
to an unresolvable discrepancy, given that physics is not prepared to handle
different speeds in the same interaction event~\cite{VeghNon-ordinaryLaws:2025}.
(Historically, Onsager's work in 1931 referred to thermoelectricity and transport phenomena in electrolytes, thus connecting experimentally two separate disciplines.)

\subsection{Locality and finite resources\label{sec:Physics-Resources}}

The closed volume of the biological objects is also
a problem of finite resources. The material transport represents simultaneously mass and charge,
so the transport itself gradually changes the gradients. 
This process keeps the entire volume of the electrolyte in (more or less intense) continuous change, which makes life possible.
The more distant parts of the biological cell will see any change in the local values of the state variables with a delay. In biology, we cannot use the
fundamental assumption of physics that, although a field acts on the 
ion in question, the ion does not affect the field (that the other ions generate).  The transferred ions decrease the field in the volume segment they departed from
and increase it in the volume segment where they arrived. Due to the 
field-dependent speed within the electrolyte, we must consider the autonomous
change between the microvolumes where the ion traverses. 
Furthermore, biological objects inside the cell
can absorb ions and charge up. With their potential, they alter local gradients, accelerating or decelerating ions.  
Not to mention that biological objects can be active in the sense that 
(depending on the environmental conditions) they can let ions from one separated volume part into the other, thus changing the number of 
charge carriers in the volume under study.

%\subsection{Finite resources\label{sec:Physics-Resources}}

\begin{advanced}
"How can \textit{the events in space and time} which take place \textbf{\textit{within the spatial boundary}} of a living organism be accounted for by physics and chemistry?"~\cite{Schrodinger:1992}
\end{advanced}
Co-existing processes may affect each other by \hypertarget{ChangingResources}{changing resources}:
changing one quantity changes the other.
Mathematics descibes such processes by linked differential equations, such as the \href{https://en.wikipedia.org/wiki/Lotka-Volterra_equations}{Lotka-Volterra} (or \hypertarget{predator_prey}{"predator-prey"}) equations.


\subsubsection{Ion channels\label{sec:Physics-ResourcesIonChannels}}


We want to describe the change of voltage due to
limited resources using an idea similar to the predator-prey model.
We have charged sheets (represented by ions in the electrolyte) with potential $U_H$ and $U_L$
on the two surfaces of the membrane, 
plus we consider 
the corresponding neighboring layers on their side toward the "bulk" of the segment. We assume that the layers'capacity is constant, so the change of charge is linearly proportional to the change of the voltage.


\begin{figure}
\iflatexml
\includegraphics[width=0.65\textwidth]{fig/IonChannelRK.svg}
\else
\includegraphics[width=0.65\textwidth]{fig/IonChannelRK.pdf}
\fi
	
	\caption{The potential values connected to the operation of ion channels. The arbitrary values of the transfer speeds (for visibility instead of approach real values) are $\alpha = 1$; 
	$\beta  =.1$; $\gamma = 0.0001$; furthermore the time scale is also arbitrary.
	\label{fig:IonChannelRK} }	
\end{figure}



We consider that a charge transfer happens from the layer with potential~$U_H$~to the layer with potential~$U_L$~
(that is, the \textit{same charge} is removed and added, respectively) with a high speed (we call it \textit{potential-accelerated} speed),
\index{potential-accelerated speed}
 furthermore, that with a much lower speed 
(we call it \textit{potential-assisted} speed)
\index{potential-assisted speed}
the charge in the neighboring layer increases and decreases, respectively, those voltages are ($U_{H-1}$~and~$U_{L+1}$). 


That is, we assume four voltages and the initial conditions
\[
U_{H-1} = U_{H} = U; \quad U_{L=1} = U_{L+1} = 0;
\]
%	
We assume a constant capacity for the layers and that the amount of transferred charge (or voltage) is proportional to the difference of voltages in the respective layers.
\[
\frac{dU_{H}}{dt} =  -\alpha * (U_H-U_L) + \beta *(U_{H-1}-U_H)
\]
%
\[
\frac{dU_{L}}{dt} = +\alpha * (U_H-U_L) - \beta *(U_{L}-U_{L+1})
\]
%
Furthermore, we assume that the after-diffusion from and to the layers next to the proximal layers is much lower than the high speed of the charge exchange
between the proximal layers, that is
\[
\frac{dU_{H-1}}{dt} = -\gamma*(U_{H-1}-U_{H})
\]
%
\[
\frac{dU_{L+1}}{dt} = +\gamma*(U_{L}-U_{L+1})
\]

As Fig.\ref{fig:IonChannelRK} depicts, voltages $U_H$ and $U_L$
quickly approach their balanced values. When they get equal, the driving force cancels.
In the meantime, the voltages $U_{H-1}$ and $U_{L+1}$ tend to approach $U_H$ and $U_L$, respectively.
 The diffusional and energy producing processes, depending on the local conditions, change the voltages
on the two ends of the channel, creating the illusion that the channel opens and closes. As~\cite{HeimburgPhysikOfNerves:2009}
analyzed, this on/off behavior of currents can be observed
for natural and synthetic lipid membranes; with and without  caps.
 In spite of slightly different experimental conditions, amplitude and typical time scale are similar.


\subsection{Complete measurement\label{sec:Physics-Complete}}

In physics, measurement means quantifying something 
relevant to the process under study.  A 'complete' measurement
measures all relevant quantities. \textit{The different disciplines of physics restrict the measured quantities to the ones which are,
	in general, that the discipline studies.
	The remaining quantities remain outside the scope of the discipline.}
The fundamental physical quantities of mechanics are length, mass, and time, which form the basis for defining all other quantities like velocity, force, and energy in that field.
The fundamental physical quantities in thermodynamics are Temperature, Energy (Internal Energy, Heat, Work), and Entropy, which characterize systems at equilibrium and describe energy transformations; Pressure and Volume are also key variables. 
Those of electricity include electrical charge (Coulomb), the basis of all electrical phenomena; electrical current (Ampere), the rate of charge flow; voltage (Volt), the potential difference driving current; resistance (Ohm), opposition to current; power (Watt), the rate of energy transfer; and energy (Joule), the capacity to do work. 
As seen, there is little overlap of the studied quantities. 

\textit{A disciplinary study (such as the electrical and thermodynamic ones in physiology) provides an incomplete set of measured quantities}. None of the disciplines alone can describe electrolytes, since they consider different and incomplete sets of physical quantities: thermodynamics misses charge, electricity misses mass.
As discussed in connection with the Onsager relations, a disciplinary (incomplete) measurement does not discover that an unexpected change (a miracle) in the value of one of the quantities belonging to another discipline is accompanied by a change in the
value of the other quantity.. This change is clearly the case with the measurements performed in the spirit of \gls{HH}: the values the
electrical instruments provide are accompanied by the values
of mechanical/thermodynamic quantities which are not measured, partly due to the obvious 
measuring difficulties and because they are outside the scope of the discipline. 
If one attempts to reduce ion-related phenomena to a single abstraction (see the mentioned theoretical descriptions),
one experiences that some quantity (the charge or the mass)
changes in an uncontrolled way.
As Feynman R. P~\cite{FeynmanThinking:1980} told, "\textit{many of the interesting phenomena bridge the gaps between fields}". The disciplinary separation in classical science does not apply to the study of living matter.


Thermodynamics provides a framework for handling ions and determining their thermodynamic properties. However,
as good thermodynamic textbooks (including the one on the thermodynamics of the membrane~\cite{ThermalBiophysics:2007}) emphasize, \textit{thermodynamics
derives its concepts for non-interacting particles}, so one cannot expect 
its validity for ionic solutions~\cite{VeghNon-ordinaryLaws:2025}
(Boltzmann assumed that, in the absence of long-range interaction between the particles, the sizes of cells in the phase space do not change).
In addition, he required  the presence of a vast number of particles
in a homogeneous, isotropic, infinite volume,
which is typically not the case in biology.

%\subsection{Complete measurement\label{sec:Physics-MeasurementComplete}}

In physics, measurement means the determination of something that is
relevant for the process under study.  A 'complete' measurement
measures all relevant quantities. The different disciplines of physics restrict the measured quantities to the ones which are,
in general, that the discipline studies.
The rest of quantities remains outside the scope of the discipline.
The fundamental physical quantities in thermodynamics are Temperature, Energy (Internal Energy, Heat, Work), and Entropy, which characterize systems at equilibrium and describe energy transformations, with Pressure and Volume also being key extensive variables. 
The fundamental physical quantities of mechanics are length, mass, and time, which form the basis for defining all other quantities like velocity, force, and energy in that field.
Those of electricity include electric charge (Coulomb), the basis of all electric phenomena; electric current (Ampere), the rate of charge flow; voltage (Volt), the potential difference driving current; resistance (Ohm), opposition to current; power (Watt), the rate of energy transfer; and energy (Joule), the capacity to do work. 
As seen, there is little overlap of the studied quantities. 
A disciplinary study (such as the electrical and thermodynamic ones in physiology)
provides an incomplete set of measured quantities, so simply neither 
of the disciplines, alone, can describe the studied phenomenon.
The fundamental reason of the incompatibility on the disciplinary theories 
is that they consider an incomplete set of physical quantities.
As discussed below in connection with the Onsager relations, a disciplinary (incomplete) measurement does not discover that an unexpected change in the value of one of the quantities is accompanied with a change of the value
in another (unmeasured) quantity. This is clearly the case with the measurements performed in the spirit of \gls{HH}: the values the
electrical instruments provide are not accompanied with the values
of mechanical quantities which are not measured, due to the obvious 
measuring difficulties. The theory, however,
should not remain disciplinary.


\subsubsection{Ions' thermodynamics\label{sec:Physics-IonsThermodynamics}}

Thermodynamics provides a framework for handling ions and determining their thermodynamic properties. However,
as good thermodynamic textbooks (including the one on the thermodynamics of the membrane~\cite{ThermalBiophysics:2007}) emphasize, thermodynamics
derives its concepts for non-interacting particles, so one cannot expect 
its validity for ionic solutions~\cite{VeghNon-ordinaryLaws:2025}
(Boltzmann assumed that, in the absence of long-range interaction between the particles, the sizes of cells in the phase space do not change).
In addition, he required  the presence of a vast number of particles
in a homogeneous, isotropic, infinite volume,
which is typically not the case in biology.


\subsection{Mathematical issue\label{sec:Physics-Mathematics}}

The interdependent behavior of charge and mass has an interesting
mathematical consequence as well. 
The definition of a partial derivative of a function of several variables is its derivative with respect to one of those variables,
\textit{with the others held constant}. In the case of ions, changing the function with respect to mass or charge means simultaneously changing also the other; that is, the partial derivatives depend on each other.
In other words, one cannot calculate the partial derivatives; only the total derivative.
As a consequence, equations that use the partial derivatives of concentration and electrical potential are either incorrect or approximations. Classical mathematics cannot be applied.
Consequently, in the case of ions and electrolytes, one must not use the well-established concepts (enthalpy, entropy, etc) of thermodynamics in unchanged form. (BTW: the discrete nature of charged particles 
leaves the question open, under which conditions remains the 
ion current differentiable.)


\subsection{Physics for biology\label{sec:Physics-Biology}}

The above items %processes
are what E.~Schr\"odinger coined as "\textit{the construction is different from anything we have yet tested in the physical laboratory}"~\cite{Schrodinger:1992}. Consequently, measurements must be designed and carried out with care; the routine
methods used for measuring objects from inanimate nature
cannot surely be applied to living objects without changing them.  

%\subsection{Physics and biology\label{sec:Physics-Biology}}

A common fallacy in biology is that \hypertarget{Physics}{physics} cannot underpin the operation of living matter, citing E.~Schrödinger. However, the claim falsifies his opinion by omitting the most essential word 'ordinary'. 
Scrödinger wanted to emphasize the opposite: there is no new force (no unknown new interaction), only studying living matter needs different testing methods in the physical laboratory. He suggested to answer the question
"Is life based on the laws of physics?” affirmatively, but expected to discover the appropriate forms of physical laws describing the 'non-ordinary' (in our reading: non-disciplinary) behavior of living matter. No doubt, the basic notions and terms must be interpreted precisely for living matter,
much beyond the level we used to at college level.
However, after that reinterpretation, we can interpret features of living matter, although we need a more careful, many-disciplinary analysis to do so. 
We must use the appropriate abstractions and approximations for the phenomena,
depending on the level needed in the given cooperation of objects and interactions.
In this section we discuss some of the relevant terms and notions of physics,
differentiating which approximation is appropriate only for physics (mainly electronics)
and,  instead, which approximation should be used for biology.
As we discuss, \textit{biophysics simply translated the corresponding major terminus technicus words
from the theory and practice of physics' major disciplines, mainly from electricity, which were
worked out for homogeneous, isotropic, structureless metals,
and for strictly pair-wise interactions with a single (actually, 'instant') interaction speed; to the structured,
\index{interaction!attributes}
non-homogeneous, non-isotropic, material mixtures and for multiple interaction speeds}.
Those notions do not always have unchanged meaning, and how much they do,
depends on the actual conditions. The precise meaning needs a case-by-case analysis.

The physical models consider infinitely large volumes,
surfaces, distances; furthermore, and most importantly: instant interactions. Is the cell large enough to
consider it infinitely large (at least on the scale using ion's size); that is to apply laws of science
derived for abstraction 'infinitely large'? 
When working with charge, we know that charge is quantized, while the macroscopic quantities voltage and current are 
continuous (derivable). Do cells contain a sufficient 
number of charge carriers to apply macroscopic notions?
Do the thousands of times smaller ion channels transfer enough charge
to speak about well-defined current?
When a couple of ions are transferred through an ion channel,
do they change significantly the potential that accelerates them?


 Science could serve as a firm base for all its disciplines.
As we discuss, \textit{its disciplines use abstractions based on limited-validity approximations} based on the same first principles.
However, \textit{the approximations are different for biology and physics.}
In physics, some processes we observe are fast enough so that we can use the %\hyperlink{Abstractions}
{abstraction} that they are essentially jumps between states.
In some cases, the approach can be --more or less-- successful. For the slower, well-observable processes,
we have the \hyperlink{PhysicsLawsOfMotion}
{laws of motion}
that describe how the processes happen under the effect of some driving force.
We also experienced that nature is not necessarily linear (in the sense that it depends only on space coordinates but not on their derivatives), which we can describe by "nice" mathematical formulas.
A century ago, A.~Einstein invented that the approximations I.~Newton introduced two centuries earlier are not sufficiently accurate for describing the movement of bodies at high speeds. In other words, a new paradigm, the constancy of the speed of light, must have been introduced
that caused a revolution in physics and led to the birth of "modern physics". 
%

\textit{Life, including the brain's operation, is dynamic.} 
As Schrödinger formulated, the "construction of living matter" differs
from the one science used to test in its labs.
The scientific abstraction based on "states" (i.e., on instant changes)
fails for the case of biology, where "processes" happen (i.e., the changes
are obviously much slower).
 The commonly used measuring methods such as \hyperlink{voltage-clamping}{clamping, patching, and freezing},  reduce the life to states (and correspondingly, 
 the related theories describe states with perturbance~\cite{PerturbationNeuralComputation:2002}). 
On the one side, this technology fixes the cell at some well-defined static state and enables us to observe a static anatomic picture of the cell. On the other,
it eliminates the dynamic processes, i.e., \textit{hides for forever the essence of the life that the cell exists in a continuous change governed by laws of motion}.
Those methods stop the processes for studying them, in this way killing their dynamicity to be measured.
It was forgotten that using feedback for stabilizing an autonomously
working electrical system means introducing foreign currents
and this way falsifying its operation. 

Furthermore, it is hazardous to introduce technically (and incorrectly) derived
and misinterpreted macroscopic features and interpret them as fundamental
electric notions. In general, instead of understanding and developing the correct scientific base of the operation, saying that science cannot describe it. The idea of
\hyperlink{electric_conductance}
{conductance}
has been introduced
to neurophysiology almost a century ago. It was taken from physics, where the
notion was derived for metals (conductors, instead of electrolytes). Since then, its original interpretation
has been forgotten, and today (in contrast with physics), it has become
a primary entity for describing electric characteristics of biological
cells. We explain how the right physics background enables us to
discover wrong physical models and 
misinterpreted notions of physics in neurophysiology;
furthermore, how the right interpretation opens the way to the correct interpretation
of neuronal information. We set up an abstract electric model of neuronal
operation.



We derive the needed 'non-ordinary laws', which are derived by using the same first principles as the 'ordinary laws', but are abstracted for the
approximations valid for living matter.
As we discuss, those 'ordinary' laws were derived for strictly pair-wise interactions at very high speeds.
In biology, we can observe interactions at million times smaller speed, in inhomogeneous, non-isotropic, structured material.
\textit{Biology has not the conditions for which we derived the ordinary laws of physics}.
We show that the ordinary laws are also the result of approximations
(including omissions) and by using the appropriate approximations for the 
biological cases we can derive those 'non-ordinary' laws of physics.
Which laws are more complex to derive and we must use several
stages (with the approximations changing from stage by stage) instead of 
one single stage in the case of the 'ordinary' laws. 
However, \textit{all laws follow the same principles}.


Biology, and especially neuronal operation, produces examples where wrong omissions
in complex processes results in absolutely wrong results. In those cases
some initial resemblance between our theoretical predictions and our phenomena exist, but the success
in simple cases provides no guarantee that the model was appropriate: "the success of the equations is no evidence in
favour of the mechanism"~\cite{HodgkinHuxley:1952}. Finally,
all laws are approximations and the accuracy of verifying their predictions is limited.
Several theories can describe the same phenomenon with the required accuracy.
We also show in the section about the 
%\hyperlink{PHYSICS_SPEEDS}
{finite interaction speeds}
 that the mostly known laws (from Newton, Coulomb, Kirchoff, etc.) are
 \index{Coulomb's law}
also approximations. They have their range of validity, although it is often
forgotten.

One such neuralgic point of omissions and approximations is  the vastly different
%\hyperlink{PHYSICS_SPEEDS_FINITE}
{interaction speeds};
furthermore, that where the speed is considered at all, \textit{the same speed is assumed for all interactions}.
The laws are abstract also in the sense that, say, the objects in the laws of physics have either mass or
electric charge, but not both. It is the researcher's task to decide
which combination of laws
can be applied to the given condition. For example, one can assume in most cases
that the speeds sum up linearly, except at very high speeds.
Biology provides excellent case studies where different interactions
shape the phenomenon and special care must be exercised.
We give a short review of 
%\hyperlink{PHYSICS_SPEEDS}
{history and kinds of interaction speeds}.

Another point is that science started with the assumption that
the non-living matter is continuous, although it was early
discovered that there are smallest pieces of that matter.
When reached that size, we experienced that different subsets
of science laws describe that matter and the atoms they contain;
in is one of the hardest tasks to establish relations between
those subsets. Again, we used abstractions that that the 
continuous matter is infinitely large and that the isolated
atoms are infinitely far from each other and from the external world. We also experienced the semi-infinite cases, and studied
the behavior of surfaces and interfaces; which, again is different from both that of the atoms and their large masses. Given that biological objects are between those microscopic and macroscopic sizes, and they are surrounded by surface, we must be prepared that no simle rules describe their behavior.

Neuronal operation is at the boundary where sometimes, in the same phenomenon,
one interaction can be interpreted at macroscopic level, some another
must already be interpreted at microscopic level.
Furthermore, a series of stages (instead of a single state) and
processes (instead of stages) describe the subject under study.
Given the vital role of %\hyperlink{electric_current}
{charge and current} in neuronal operation,
we give their precise interpretation.
Furthermore, we must consider that the processes happen in a finite volume,
"\hyperlink{ChangingResources}
{within the spatial boundary}".
Special emphasis is given to the true interpretation of 
\hyperlink{electric_conductance}
{conductance},
one of the central terms in biology.


